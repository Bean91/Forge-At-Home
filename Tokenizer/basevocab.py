import emoji
import string

letters = list(string.ascii_letters)
digits = list(string.digits)
whitespace = [' ', '\n', '\t']

keyboard_symbols = list(string.punctuation)
extra_symbols = [
    'â€œ', 'â€', 'â€˜', 'â€™', 'â€“', 'â€”', 'â€¦',
    'â€¢', 'Â·', 'Â§', 'Â¶', 'Â©', 'Â®', 'â„¢',
    'Â°', 'Â±', 'Ã—', 'Ã·', 'Âµ', 'Î©', 'âˆ',
    'â†', 'â†’', 'â†‘', 'â†“', 'â†”', 'â†•',
    'â˜…', 'â˜†', 'âœ“', 'âœ”', 'âœ•', 'âœ–', 'âœ—', 'âœ˜',
    'â˜€', 'â˜', 'â˜‚', 'â˜ƒ', 'â˜„', 'â™ ', 'â™£', 'â™¥', 'â™¦',
    'â›”', 'âš ', 'âš¡', 'â³', 'âŒ›', 'ğŸ”’', 'ğŸ”“', 'ğŸ”‘', 'ğŸ—ï¸'
]

all_emojis = list(emoji.EMOJI_DATA.keys())

base_tokens = letters + digits + whitespace + keyboard_symbols + extra_symbols + all_emojis

ase_tokens = list(dict.fromkeys(base_tokens))

vocab_path = "Tokenizer/vocab.txt"
with open(vocab_path, "w", encoding="utf-8") as f:
    for token in base_tokens:
        f.write(token + "\n")
